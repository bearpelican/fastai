{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.models.cifar10.wideresnet import wrn_22_cat, wrn_22, WideResNetConcat\n",
    "torch.backends.cudnn.benchmark = True\n",
    "PATH = Path(\"data/cifar10/\")\n",
    "os.makedirs(PATH,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))\n",
    "\n",
    "bs=1024\n",
    "sz=32\n",
    "workers=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "def pad(img, p=4, padding_mode='reflect'):\n",
    "    return Image.fromarray(np.pad(np.asarray(img), ((p, p), (p, p), (0, 0)), padding_mode))\n",
    "\n",
    "def torch_loader(data_path, size, prefetcher=True):\n",
    "    if not os.path.exists(data_path/'train'): download_cifar10(data_path)\n",
    "\n",
    "    # Data loading code\n",
    "    traindir = str(data_path/'train')\n",
    "    valdir = str(data_path/'test')\n",
    "    tfms = [transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]\n",
    "\n",
    "    train_tfms = transforms.Compose([\n",
    "        pad, # TODO: use `padding` rather than assuming 4\n",
    "        transforms.RandomCrop(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tfms)\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(traindir, train_tfms)\n",
    "    val_dataset = datasets.ImageFolder(valdir, transforms.Compose(tfms))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=bs, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "    \n",
    "    aug_loader = DataLoader(\n",
    "        datasets.ImageFolder(valdir, train_tfms),\n",
    "        batch_size=bs, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    if prefetcher:\n",
    "        train_loader = DataPrefetcher(train_loader)\n",
    "        val_loader = DataPrefetcher(val_loader)\n",
    "        aug_loader = DataPrefetcher(aug_loader)\n",
    "    \n",
    "    data = ModelData(data_path, train_loader, val_loader)\n",
    "    data.sz = size\n",
    "    data.aug_dl = aug_loader\n",
    "    return data\n",
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch_loader(PATH, sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pre-activation ResNet in PyTorch.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    def __init__(self, sz=None):\n",
    "        super().__init__()\n",
    "        sz = sz or (1,1)\n",
    "        self.ap = nn.AdaptiveAvgPool2d(sz)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(sz)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "    \n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.bn2.bias.data.zero_()\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x), inplace=True)\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out), inplace=True))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, concatpool=False):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.pool = AdaptiveConcatPool2d() if concatpool else nn.AdaptiveMaxPool2d((1,1))\n",
    "        \n",
    "        self.linear = nn.Linear(512*block.expansion*(concatpool+1), num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "#         out = F.adaptive_max_pool2d(out, 1)\n",
    "        out = self.pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return F.log_softmax(self.linear(out))\n",
    "\n",
    "def preact_resnet18(): return PreActResNet(PreActBlock, [2,2,2,2])\n",
    "def preact_resnet2332(): return PreActResNet(PreActBlock, [2,3,3,2])\n",
    "def preact_resnet3333(): return PreActResNet(PreActBlock, [3,3,3,3])\n",
    "def preact_resnet34(): return PreActResNet(PreActBlock, [3,4,6,3])\n",
    "def preact_resnet50(): return PreActResNet(PreActBottleneck, [3,4,6,3])\n",
    "def preActResNet101(): return PreActResNet(PreActBottleneck, [3,4,23,3])\n",
    "def preActResNet152(): return PreActResNet(PreActBottleneck, [3,8,36,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = WideResNetConcat(num_groups=3, N=3, num_classes=10, k=1, drop_p=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TTA_accuracy(learn):\n",
    "    preds, targs = learn.TTA()\n",
    "    # combining the predictions across augmented and non augmented inputs\n",
    "    preds = 0.6 * preds[0] + 0.4 * preds[1:].sum(0)\n",
    "    return accuracy_np(preds, targs)\n",
    "\n",
    "def get_TTA_accuracy_2(learn):\n",
    "    log_preds,y = learn.TTA()\n",
    "    preds = np.mean(np.exp(log_preds),0)\n",
    "    acc = accuracy(torch.FloatTensor(preds),torch.LongTensor(y))\n",
    "    print('TTA acc:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74060a53b0694f25b61f77b2f1462319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=26), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.559481   1.375577   0.494     \n",
      "    1      1.214859   1.258109   0.5722                   \n",
      "    2      0.98733    1.075784   0.6534                    \n",
      "    3      0.851114   1.346923   0.6442                    \n",
      "    4      0.722213   1.100872   0.6906                    \n",
      "    5      0.624781   0.689455   0.7868                    \n",
      "    6      0.563165   0.756333   0.7731                    \n",
      "    7      0.522025   0.709459   0.7825                    \n",
      "    8      0.478817   0.630427   0.7977                    \n",
      "    9      0.427421   0.535656   0.8182                    \n",
      "    10     0.393279   0.941095   0.7433                    \n",
      "    11     0.372102   0.756953   0.7801                    \n",
      "    12     0.356929   0.435244   0.8555                    \n",
      "    13     0.294046   0.357349   0.884                     \n",
      "    14     0.245216   0.412861   0.8711                    \n",
      "    15     0.20782    0.334846   0.8983                    \n",
      "    16     0.177676   0.332271   0.8993                    \n",
      "    17     0.150658   0.305392   0.9059                    \n",
      "    18     0.12521    0.313163   0.9082                    \n",
      "    19     0.099487   0.30058    0.9142                     \n",
      "    20     0.076819   0.284375   0.9206                     \n",
      "    21     0.059221   0.268565   0.9291                     \n",
      "    22     0.046928   0.259488   0.93                       \n",
      "    23     0.039402   0.261916   0.93                       \n",
      "    24     0.033489   0.25997    0.9318                     \n",
      "    25     0.02932    0.260223   0.9328                     \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.260223046875, 0.9327999989509582]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=5e-4\n",
    "lr=1e-3\n",
    "learn.clip = 3e-1\n",
    "\n",
    "def_phase = {'opt_fn':optim.Adam, 'wds':wd}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=2, lr=(1e-4, 1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=10, lr=(1e-3,1e-2), lr_decay=DecayType.LINEAR, momentum=(0.95,0.85), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=10, lr=(1e-2,1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=4, lr=(1e-3,1e-5), lr_decay=DecayType.LINEAR, momentum=(0.95,0.98), momentum_decay=DecayType.LINEAR, wd_loss=False)\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases, loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb39bfa5569a47b5b10a1c5fd34957d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=26), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.518948   2.029938   0.3991    \n",
      "    1      1.204774   1.092209   0.6287                   \n",
      "    2      0.968324   1.203947   0.608                     \n",
      "    3      0.823453   1.125828   0.6542                    \n",
      "    4      0.713277   1.419574   0.6458                    \n",
      "    5      0.63577    1.110034   0.6782                    \n",
      "    6      0.569573   0.858042   0.7264                    \n",
      "    7      0.512871   0.714342   0.767                     \n",
      "    8      0.468518   0.972811   0.7007                    \n",
      "    9      0.434033   0.677549   0.7749                    \n",
      "    10     0.403709   0.594791   0.8069                    \n",
      "    11     0.395056   0.605798   0.809                     \n",
      "    12     0.360575   0.507203   0.8432                    \n",
      "    13     0.336032   0.437429   0.8599                    \n",
      "    14     0.279051   0.442298   0.8669                    \n",
      "    15     0.240927   0.377872   0.8849                    \n",
      "    16     0.206928   0.333829   0.8972                    \n",
      "    17     0.170284   0.315015   0.9034                    \n",
      "    18     0.143077   0.339638   0.9074                    \n",
      "    19     0.11762    0.306675   0.9082                    \n",
      "    20     0.091833   0.286698   0.921                      \n",
      "    21     0.071218   0.284916   0.9223                     \n",
      "    22     0.056058   0.274473   0.9262                     \n",
      "    23     0.046356   0.275907   0.926                      \n",
      "    24     0.038855   0.276654   0.9272                     \n",
      "    25     0.034928   0.280069   0.9268                     \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2800693359375, 0.9268000018119812]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=5e-4\n",
    "lr=1e-3\n",
    "learn.clip = 3e-1\n",
    "\n",
    "def_phase = {'opt_fn':optim.Adam, 'wds':wd}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=2, lr=(2e-4, 2e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=10, lr=(2e-3,2e-2), lr_decay=DecayType.LINEAR, momentum=(0.95,0.85), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=10, lr=(2e-2,2e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=4, lr=(2e-3,1e-5), lr_decay=DecayType.LINEAR, momentum=(0.95,0.98), momentum_decay=DecayType.LINEAR, wd_loss=False)\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases, loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6270d2d4c1b4162a44d026785d18ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=34), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.55699    1.578994   0.4194    \n",
      "    1      1.21989    1.157563   0.6014                   \n",
      "    2      0.990838   1.016119   0.6649                    \n",
      "    3      0.830221   0.946743   0.6957                    \n",
      "    4      0.712191   0.817422   0.7304                    \n",
      "    5      0.641922   0.723941   0.7628                    \n",
      "    6      0.588504   0.621541   0.7904                    \n",
      "    7      0.543973   1.61429    0.6102                    \n",
      "    8      0.515282   0.738418   0.765                     \n",
      "    9      0.464268   0.784556   0.7783                    \n",
      "    10     0.428416   0.890738   0.7562                    \n",
      "    11     0.404087   1.065597   0.7035                    \n",
      "    12     0.387005   0.783281   0.7591                    \n",
      "    13     0.35338    0.517973   0.8383                    \n",
      "    14     0.322273   0.474601   0.8458                    \n",
      "    15     0.310378   0.796406   0.7807                    \n",
      "    16     0.288643   0.553945   0.8377                    \n",
      "    17     0.244358   0.461771   0.8529                    \n",
      "    18     0.209734   0.381185   0.8816                    \n",
      "    19     0.184375   0.386816   0.8813                    \n",
      "    20     0.161176   0.393448   0.8862                    \n",
      "    21     0.134269   0.308759   0.9099                    \n",
      "    22     0.110056   0.348763   0.8988                    \n",
      "    23     0.092349   0.332609   0.9077                     \n",
      "    24     0.074184   0.307071   0.9183                     \n",
      "    25     0.059476   0.283122   0.9235                     \n",
      "    26     0.047183   0.280357   0.9274                     \n",
      "    27     0.035743   0.282698   0.9273                     \n",
      "    28     0.028087   0.279883   0.9298                     \n",
      "    29     0.022768   0.280742   0.9308                     \n",
      "    30     0.019484   0.285578   0.9301                     \n",
      "    31     0.017075   0.288958   0.9308                     \n",
      "    32     0.014703   0.289728   0.9309                     \n",
      "    33     0.013509   0.289305   0.9308                     \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2893052734375, 0.9308000016212463]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=5e-4\n",
    "lr=1e-3\n",
    "learn.clip = 3e-1\n",
    "\n",
    "def_phase = {'opt_fn':optim.Adam, 'wds':wd}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=4, lr=(1e-4, 1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=12, lr=(1e-3,1e-2), lr_decay=DecayType.LINEAR, momentum=(0.95,0.85), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=12, lr=(1e-2,1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=6, lr=(1e-3,1e-5), lr_decay=DecayType.LINEAR, momentum=(0.95,0.98), momentum_decay=DecayType.LINEAR, wd_loss=False)\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases, loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be879b4f77964bf3b9c1e53de893c5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.546197   1.556898   0.4582    \n",
      "    1      1.200931   1.157685   0.6213                   \n",
      " 98%|█████████▊| 48/49 [00:07<00:00,  6.69it/s, loss=1.01]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type Half without overflow: inf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-df014f935cf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m ]\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_opt_sched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastai/courses/dl2/fastai/learner.py\u001b[0m in \u001b[0;36mfit_opt_sched\u001b[0;34m(self, phases, cycle_save_name, best_save_name, stop_div, data_list, callbacks, cut, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_crit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl2/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl2/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y, epoch)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp32_params\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHalfTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mIS_TORCH_04\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: value cannot be converted to type Half without overflow: inf"
     ]
    }
   ],
   "source": [
    "m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=5e-4\n",
    "lr=1e-3\n",
    "learn.clip = 3e-1\n",
    "\n",
    "def_phase = {'opt_fn':optim.Adam, 'wds':wd}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=2, lr=(1e-4, 1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=12, lr=(1e-3,4e-2), lr_decay=DecayType.LINEAR, momentum=(0.95,0.85), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=12, lr=(4e-2,1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=4, lr=(1e-3,1e-5), lr_decay=DecayType.LINEAR, momentum=(0.95,0.98), momentum_decay=DecayType.LINEAR, wd_loss=False)\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases, loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccbb97e58d241c09db7a1c7cfba450b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.547924   1.418103   0.4774    \n",
      "    1      1.210919   1.399253   0.5572                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.399253125, 0.5572]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-3\n",
    "lr=1e-3\n",
    "learn.clip = .5\n",
    "\n",
    "def_phase = {'opt_fn':optim.Adam, 'wds':wd}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=2, lr=(1e-4, 1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False)\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases, loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9c732b77334196a4174d863bb597ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.552995   1.385894   0.4987    \n",
      "    1      1.204416   1.219089   0.5984                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.219088671875, 0.5984000007629394]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-3\n",
    "lr=1e-3\n",
    "\n",
    "def_phase = {'opt_fn':optim.Adam, 'wds':wd}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=2, lr=(1e-4, 1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False)\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases, loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=5e-4\n",
    "lr=1e-3\n",
    "learn.clip = 3e-1\n",
    "\n",
    "def_phase = {'opt_fn':optim.Adam, 'wds':wd}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=2, lr=(1e-4, 1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False)\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases, loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=5e-4\n",
    "lr=1e-3\n",
    "learn.clip = 3e-1\n",
    "\n",
    "def_phase = {'opt_fn':optim.Adam, 'wds':wd}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=2, lr=(1e-4, 1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=10, lr=(1e-3,1e-2), lr_decay=DecayType.LINEAR, momentum=(0.95,0.85), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=10, lr=(1e-2,1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=4, lr=(1e-3,1e-5), lr_decay=DecayType.LINEAR, momentum=(0.95,0.98), momentum_decay=DecayType.LINEAR, wd_loss=False)\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases, loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-3\n",
    "lr=1e-3\n",
    "learn.clip = 3e-1\n",
    "\n",
    "def_phase = {'opt_fn':optim.Adam, 'wds':wd}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=2, lr=(1e-4, 1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=10, lr=(1e-3,1e-2), lr_decay=DecayType.LINEAR, momentum=(0.95,0.85), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=10, lr=(1e-2,1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=4, lr=(1e-3,1e-5), lr_decay=DecayType.LINEAR, momentum=(0.95,0.98), momentum_decay=DecayType.LINEAR, wd_loss=False)\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases, loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-4\n",
    "lr=1e-3\n",
    "learn.clip = 3e-1\n",
    "\n",
    "def_phase = {'opt_fn':optim.Adam, 'wds':wd}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=2, lr=(1e-4, 1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=10, lr=(1e-3,1e-2), lr_decay=DecayType.LINEAR, momentum=(0.95,0.85), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=10, lr=(1e-2,1e-3), lr_decay=DecayType.LINEAR, momentum=(0.85,0.95), momentum_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=4, lr=(1e-3,1e-5), lr_decay=DecayType.LINEAR, momentum=(0.95,0.98), momentum_decay=DecayType.LINEAR, wd_loss=False)\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases, loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('att6-tta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = [TrainingPhase(**def_phase, epochs=4, lr=(.04,.001), lr_decay=DecayType.LINEAR, momentum=(0.95))]\n",
    "learn.fit_opt_sched(phases, data_list=[data], loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_data = torch_loader(PATH, sz, prefetcher=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data_ = tta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_TTA_accuracy(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_TTA_accuracy_2(learn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "266px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
