{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "# from fastai.models.cifar10.wideresnet import wrn_22_cat, wrn_22, WideResNetConcat\n",
    "torch.backends.cudnn.benchmark = True\n",
    "PATH = Path.home()/\"data/cifar10/\"\n",
    "os.makedirs(PATH,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace/fastai/courses/dl2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))\n",
    "workers=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "def pad(img, p=4, padding_mode='reflect'):\n",
    "    return Image.fromarray(np.pad(np.asarray(img), ((p, p), (p, p), (0, 0)), padding_mode))\n",
    "\n",
    "def torch_loader(data_path, size, bs, val_bs=None, prefetcher=True):\n",
    "#     if not os.path.exists(data_path/'train'): download_cifar10(data_path)\n",
    "\n",
    "    val_bs = val_bs or bs\n",
    "    # Data loading code\n",
    "    traindir = str(data_path/'train')\n",
    "    valdir = str(data_path/'test')\n",
    "    tfms = [transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]\n",
    "\n",
    "    train_tfms = transforms.Compose([\n",
    "        pad, # TODO: use `padding` rather than assuming 4\n",
    "        transforms.RandomCrop(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tfms)\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(traindir, train_tfms)\n",
    "    val_dataset = datasets.ImageFolder(valdir, transforms.Compose(tfms))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=val_bs, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "    \n",
    "    aug_loader = DataLoader(\n",
    "        datasets.ImageFolder(valdir, train_tfms),\n",
    "        batch_size=bs, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    if prefetcher:\n",
    "        train_loader = DataPrefetcher(train_loader)\n",
    "        val_loader = DataPrefetcher(val_loader)\n",
    "        aug_loader = DataPrefetcher(aug_loader)\n",
    "    \n",
    "    data = ModelData(data_path, train_loader, val_loader)\n",
    "    data.sz = size\n",
    "    data.aug_dl = aug_loader\n",
    "    return data\n",
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = WideResNetConcat(num_groups=3, N=3, num_classes=10, k=1, drop_p=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.models.cifar10.wideresnet import wrn_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b39445f212e4711b6e9a679a81e2c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.137288   1.385104   0.5512    \n",
      "    1      0.837653   0.816871   0.7248                      \n",
      "    2      0.649919   0.823135   0.7351                      \n",
      "    3      0.567991   0.980156   0.7042                      \n",
      "    4      0.514481   0.502497   0.826                       \n",
      "    5      0.470965   0.757897   0.7564                      \n",
      "    6      0.438445   0.548408   0.8212                      \n",
      "    7      0.414006   0.632729   0.7944                      \n",
      "    8      0.392887   0.457804   0.8457                      \n",
      "    9      0.387933   0.451746   0.8473                      \n",
      "    10     0.390795   0.458801   0.8485                      \n",
      "    11     0.37432    0.649176   0.7969                      \n",
      "    12     0.341124   0.512372   0.8353                      \n",
      "    13     0.318512   0.571917   0.8077                      \n",
      "    14     0.315084   0.453722   0.8449                      \n",
      "    15     0.31077    0.420123   0.8571                      \n",
      "    16     0.293236   0.666652   0.792                       \n",
      "    17     0.26862    0.378018   0.8742                      \n",
      "    18     0.256914   0.396176   0.8654                      \n",
      "    19     0.239071   0.429669   0.8646                      \n",
      "    20     0.218942   0.353059   0.8862                      \n",
      "    21     0.185801   0.296854   0.9018                      \n",
      "    22     0.137587   0.2662     0.9164                      \n",
      "    23     0.08727    0.209238   0.9318                       \n",
      "    24     0.060806   0.206015   0.9349                       \n",
      "    25     0.052452   0.205843   0.9377                       \n",
      "    26     0.040699   0.209816   0.9367                       \n",
      "    27     0.037299   0.205851   0.9383                       \n",
      "    28     0.027022   0.207786   0.9385                       \n",
      "    29     0.024108   0.205735   0.9402                       \n",
      "\n",
      "CPU times: user 6min 49s, sys: 1min 57s, total: 8min 47s\n",
      "Wall time: 9min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2057345703125, 0.9401999992370605]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=128\n",
    "sz=32\n",
    "data = torch_loader(PATH, sz, bs, 512)\n",
    "\n",
    "# m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "# m = ResNet18()\n",
    "m = wrn_22()\n",
    "\n",
    "# m = FP16(m.cuda())\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = F.cross_entropy\n",
    "# learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-4\n",
    "lr=.5\n",
    "# learn.clip = 1e-2\n",
    "\n",
    "# learn.opt_fn = partial(optim.SGD, nesterov=True, momentum=0.9)\n",
    "\n",
    "# %time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(20,20,0.95,0.85))\n",
    "%time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(20,20,0.95,0.85), loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5774676bc0094349ae7d0e21fbbe7e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.225538   1.17062    0.5796    \n",
      "    1      0.931035   1.10479    0.6262                      \n",
      "    2      0.771767   0.754736   0.7346                      \n",
      "    3      0.667968   0.804059   0.7344                      \n",
      "    4      0.570667   0.68123    0.7677                      \n",
      "    5      0.524218   0.626479   0.7895                      \n",
      "    6      0.498802   0.580072   0.8062                      \n",
      "    7      0.459822   0.541853   0.8185                      \n",
      "    8      0.425889   0.50452    0.8329                      \n",
      "    9      0.414437   0.511546   0.8257                      \n",
      "    10     0.417682   0.481747   0.841                       \n",
      "    11     0.368253   0.529707   0.8303                      \n",
      "    12     0.363285   0.591126   0.8169                      \n",
      "    13     0.366431   0.471198   0.841                       \n",
      "    14     0.318771   0.398132   0.8681                      \n",
      "    15     0.291618   0.418959   0.8626                      \n",
      "    16     0.268864   0.369209   0.8803                      \n",
      "    17     0.253578   0.433789   0.8585                      \n",
      "    18     0.220819   0.326845   0.8912                      \n",
      "    19     0.196448   0.304151   0.9017                      \n",
      "    20     0.168589   0.267901   0.9119                      \n",
      "    21     0.15677    0.264307   0.9174                      \n",
      "    22     0.135262   0.262625   0.9177                      \n",
      "    23     0.107636   0.27479    0.9178                      \n",
      "    24     0.089671   0.258906   0.9219                       \n",
      "    25     0.071635   0.244093   0.9281                       \n",
      "    26     0.057648   0.244688   0.9305                       \n",
      "    27     0.039918   0.242377   0.9348                       \n",
      "    28     0.033422   0.234769   0.9339                       \n",
      "    29     0.023898   0.23644    0.9358                       \n",
      "\n",
      "CPU times: user 7min 12s, sys: 1min 53s, total: 9min 5s\n",
      "Wall time: 9min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2364404296875, 0.9358000004768372]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=128\n",
    "sz=32\n",
    "data = torch_loader(PATH, sz, bs, 512)\n",
    "\n",
    "# m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "# m = ResNet18()\n",
    "m = wrn_22()\n",
    "\n",
    "# m = FP16(m.cuda())\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = torch.nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-4\n",
    "lr = 1e-3\n",
    "# learn.clip = 1e-2\n",
    "\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "\n",
    "# %time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85))\n",
    "%time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a2540b59134c328e3457be4c35463e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.112738   1.07002    0.6218    \n",
      "    1      0.879547   1.095173   0.6402                      \n",
      "    2      0.731016   0.783476   0.7252                      \n",
      "    3      0.620737   0.639853   0.7835                      \n",
      "    4      0.579675   0.732586   0.7497                      \n",
      "    5      0.548672   0.598489   0.8019                      \n",
      "    6      0.515663   0.624616   0.7836                      \n",
      "    7      0.488925   0.712434   0.768                       \n",
      "    8      0.481206   0.674869   0.7749                      \n",
      "    9      0.454063   0.618895   0.8017                      \n",
      "    10     0.443535   0.927334   0.7237                      \n",
      "    11     0.434211   0.662073   0.782                       \n",
      "    12     0.436364   0.527627   0.8264                      \n",
      "    13     0.416736   0.444073   0.8533                      \n",
      "    14     0.390637   0.423986   0.8596                      \n",
      "    15     0.370236   0.48105    0.8405                      \n",
      "    16     0.345907   0.398508   0.8628                      \n",
      "    17     0.311251   0.468709   0.8505                      \n",
      "    18     0.301227   0.373375   0.8788                      \n",
      "    19     0.2696     0.356035   0.8798                      \n",
      "    20     0.251771   0.322469   0.8918                      \n",
      "    21     0.222987   0.278388   0.907                       \n",
      "    22     0.207181   0.273779   0.9101                      \n",
      "    23     0.183987   0.281449   0.9078                      \n",
      "    24     0.16596    0.288865   0.9081                      \n",
      "    25     0.143977   0.26607    0.9156                      \n",
      "    26     0.119521   0.255478   0.9218                      \n",
      "    27     0.089218   0.245235   0.9263                       \n",
      "    28     0.08296    0.239867   0.9285                       \n",
      "    29     0.064246   0.24093    0.9287                       \n",
      "\n",
      "CPU times: user 7min 3s, sys: 2min 3s, total: 9min 7s\n",
      "Wall time: 9min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.240930078125, 0.9287]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=128\n",
    "sz=32\n",
    "data = torch_loader(PATH, sz, bs, 512)\n",
    "\n",
    "# m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "# m = ResNet18()\n",
    "m = wrn_22()\n",
    "\n",
    "# m = FP16(m.cuda())\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = torch.nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-4\n",
    "lr = 3e-3\n",
    "# learn.clip = 1e-2\n",
    "\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "\n",
    "# %time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85))\n",
    "%time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393aafff688642b1bb2f9c839e0e2568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.083976   1.257236   0.5664    \n",
      "    1      0.824857   0.992344   0.6511                      \n",
      "    2      0.669112   0.697577   0.7641                      \n",
      "    3      0.601613   0.690393   0.7651                      \n",
      "    4      0.533504   0.57261    0.8082                      \n",
      "    5      0.472117   0.515276   0.8264                      \n",
      "    6      0.437557   0.522781   0.8267                      \n",
      "    7      0.406608   0.485396   0.8367                      \n",
      "    8      0.371274   0.431782   0.8596                      \n",
      "    9      0.357207   0.4434     0.85                        \n",
      "    10     0.334065   0.368199   0.878                       \n",
      "    11     0.304465   0.518328   0.8437                      \n",
      "    12     0.284924   0.552292   0.8254                      \n",
      "    13     0.280122   0.362412   0.8813                      \n",
      "    14     0.246771   0.397428   0.8786                      \n",
      "    15     0.209029   0.33879    0.8988                      \n",
      "    16     0.19076    0.325706   0.902                       \n",
      "    17     0.160049   0.322327   0.9017                      \n",
      "    18     0.137227   0.297922   0.909                       \n",
      "    19     0.11618    0.30043    0.9135                      \n",
      "    20     0.09609    0.276624   0.9215                       \n",
      "    21     0.091733   0.345284   0.9142                       \n",
      "    22     0.06437    0.31937    0.9197                       \n",
      "    23     0.055456   0.292937   0.925                        \n",
      "    24     0.038538   0.280731   0.932                        \n",
      "    25     0.032966   0.289777   0.932                        \n",
      "    26     0.020125   0.309646   0.9351                       \n",
      "    27     0.018292   0.290459   0.9363                       \n",
      "    28     0.012194   0.291534   0.9377                       \n",
      "    29     0.009933   0.292288   0.9384                        \n",
      "\n",
      "CPU times: user 7min 14s, sys: 1min 53s, total: 9min 7s\n",
      "Wall time: 9min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.292287890625, 0.9383999992370605]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=128\n",
    "sz=32\n",
    "data = torch_loader(PATH, sz, bs, 512)\n",
    "\n",
    "# m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "# m = ResNet18()\n",
    "m = wrn_22()\n",
    "\n",
    "# m = FP16(m.cuda())\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = F.cross_entropy\n",
    "# learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-4\n",
    "lr=3e-3\n",
    "# learn.clip = 1e-2\n",
    "\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "\n",
    "%time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), use_wd_sched=True, loss_scale=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own adam otpimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(torch.optim.Optimizer):\n",
    "    \"\"\"Implements Adam algorithm.\n",
    "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "#                 if group['weight_decay'] != 0:\n",
    "#                     grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "                \n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "                \n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000f440fcc584dc59a4495e03bd75ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.125994   1.18783    0.577     \n",
      "    1      0.86175    0.988443   0.6599                      \n",
      "    2      0.726343   0.688727   0.7625                      \n",
      "    3      0.608317   0.640625   0.7845                      \n",
      "    4      0.534234   0.622087   0.787                       \n",
      "    5      0.507067   0.515154   0.8278                      \n",
      "    6      0.475229   0.533281   0.8187                      \n",
      "    7      0.425627   0.524897   0.8208                      \n",
      "    8      0.393281   0.468985   0.8407                      \n",
      "    9      0.366347   0.44692    0.8448                      \n",
      "    10     0.33129    0.484046   0.8456                      \n",
      "    11     0.337636   0.442385   0.8597                      \n",
      "    12     0.302043   0.566451   0.8247                      \n",
      "    13     0.294848   0.397648   0.8679                      \n",
      "    14     0.267296   0.37949    0.8755                      \n",
      "    15     0.252204   0.325117   0.891                       \n",
      "    16     0.209182   0.294329   0.9045                      \n",
      "    17     0.191771   0.320806   0.8999                      \n",
      "    18     0.151801   0.366651   0.8935                      \n",
      "    19     0.138045   0.328498   0.9008                      \n",
      "    20     0.121347   0.344724   0.9079                      \n",
      "    21     0.109499   0.278453   0.9171                       \n",
      "    22     0.082704   0.25984    0.9267                       \n",
      "    23     0.079545   0.262149   0.9277                       \n",
      "    24     0.057077   0.256735   0.9303                       \n",
      "    25     0.039654   0.268596   0.9294                       \n",
      "    26     0.033433   0.247217   0.9361                       \n",
      "    27     0.019635   0.247843   0.9379                       \n",
      "    28     0.015198   0.23813    0.9416                       \n",
      "    29     0.012462   0.225477   0.9407                       \n",
      "\n",
      "CPU times: user 7min 2s, sys: 2min 5s, total: 9min 8s\n",
      "Wall time: 9min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2254771484375, 0.940700000667572]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=128\n",
    "sz=32\n",
    "data = torch_loader(PATH, sz, bs, 512)\n",
    "\n",
    "# m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "# m = ResNet18()\n",
    "m = wrn_22()\n",
    "\n",
    "# m = FP16(m.cuda())\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = F.cross_entropy\n",
    "# learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-4\n",
    "lr=3e-3\n",
    "# learn.clip = 1e-2\n",
    "\n",
    "learn.opt_fn = partial(AdamW, betas=(0.95,0.99))\n",
    "\n",
    "%time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWLR(torch.optim.Optimizer):\n",
    "    \"\"\"Implements Adam algorithm.\n",
    "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "#                 if group['weight_decay'] != 0:\n",
    "#                     grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay']*group['lr'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "                \n",
    "                \n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d806b7899b344869a28eb6d5bec30fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.103384   1.053332   0.629     \n",
      "    1      0.848033   1.030427   0.6593                      \n",
      "    2      0.685922   0.71512    0.7574                      \n",
      "    3      0.60255    0.70709    0.761                       \n",
      "    4      0.513094   0.495721   0.8335                      \n",
      "    5      0.4785     0.570843   0.8131                      \n",
      "    6      0.445733   0.589849   0.8115                      \n",
      "    7      0.410978   0.422468   0.8532                      \n",
      "    8      0.377846   0.457348   0.8462                      \n",
      "    9      0.344275   0.563315   0.822                       \n",
      "    10     0.340491   0.437142   0.8626                      \n",
      "    11     0.307948   0.466289   0.8481                      \n",
      "    12     0.307655   0.395662   0.8743                      \n",
      "    13     0.283491   0.435071   0.8646                      \n",
      "    14     0.251735   0.339573   0.895                       \n",
      "    15     0.200418   0.321469   0.9                         \n",
      "    16     0.183571   0.387569   0.8856                      \n",
      "    17     0.15876    0.311049   0.9076                      \n",
      "    18     0.139098   0.281439   0.9129                      \n",
      "    19     0.120848   0.307724   0.9133                      \n",
      "    20     0.096507   0.289654   0.9193                       \n",
      "    21     0.079622   0.283834   0.926                        \n",
      "    22     0.070988   0.295605   0.9262                       \n",
      "    23     0.057602   0.292723   0.9259                       \n",
      "    24     0.038625   0.30232    0.9287                       \n",
      "    25     0.031516   0.286667   0.9314                       \n",
      "    26     0.022188   0.295964   0.9332                       \n",
      "    27     0.015014   0.313439   0.9353                       \n",
      "    28     0.012434   0.30584    0.9378                        \n",
      "    29     0.011278   0.300378   0.9385                        \n",
      "\n",
      "CPU times: user 7min 4s, sys: 2min 4s, total: 9min 9s\n",
      "Wall time: 9min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.300377734375, 0.9385000005722046]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=128\n",
    "sz=32\n",
    "data = torch_loader(PATH, sz, bs, 512)\n",
    "\n",
    "# m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "# m = ResNet18()\n",
    "m = wrn_22()\n",
    "\n",
    "# m = FP16(m.cuda())\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = F.cross_entropy\n",
    "# learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-4\n",
    "lr=3e-3\n",
    "# learn.clip = 1e-2\n",
    "\n",
    "learn.opt_fn = partial(AdamWLR, betas=(0.95,0.99))\n",
    "\n",
    "%time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), loss_scale=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tryin AdamW again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.08179    1.31085    0.528     \n",
      "    1      0.823981   0.978336   0.6742                      \n",
      "    2      0.685794   0.725625   0.7482                      \n",
      "    3      0.574724   0.820043   0.7318                      \n",
      "    4      0.534792   0.635923   0.7889                      \n",
      "    5      0.477364   0.596087   0.7975                      \n",
      "    6      0.471159   0.535218   0.8201                      \n",
      "    7      0.430182   0.420409   0.8552                      \n",
      "    8      0.387191   0.772662   0.7687                      \n",
      "    9      0.373985   0.440674   0.8525                      \n",
      "    10     0.357146   0.433321   0.857                       \n",
      "    11     0.330867   0.389621   0.8711                      \n",
      "    12     0.313393   0.468736   0.8486                      \n",
      "    13     0.307693   0.377807   0.8741                      \n",
      "    14     0.283999   0.366225   0.8765                      \n",
      "    15     0.234759   0.369906   0.8828                      \n",
      "    16     0.22216    0.317153   0.8996                      \n",
      "    17     0.191835   0.35759    0.8908                      \n",
      "    18     0.156479   0.275783   0.9114                      \n",
      "    19     0.14183    0.325489   0.9019                      \n",
      "    20     0.125971   0.275029   0.9167                      \n",
      "    21     0.11198    0.328392   0.9067                       \n",
      "    22     0.098718   0.307152   0.9124                       \n",
      "    23     0.0665     0.266373   0.9268                       \n",
      "    24     0.062258   0.25627    0.9281                       \n",
      "    25     0.045572   0.251175   0.9338                       \n",
      "    26     0.032519   0.252909   0.9325                       \n",
      "    27     0.020806   0.248804   0.9373                       \n",
      "    28     0.013742   0.243514   0.9383                       \n",
      "    29     0.012826   0.233418   0.9395                       \n",
      "\n",
      "CPU times: user 7min 4s, sys: 2min 4s, total: 9min 8s\n",
      "Wall time: 9min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23341826171875, 0.939500000667572]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=128\n",
    "sz=32\n",
    "data = torch_loader(PATH, sz, bs, 512)\n",
    "\n",
    "# m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "# m = ResNet18()\n",
    "m = wrn_22()\n",
    "\n",
    "# m = FP16(m.cuda())\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = F.cross_entropy\n",
    "# learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-4\n",
    "lr=3e-3\n",
    "# learn.clip = 1e-2\n",
    "\n",
    "learn.opt_fn = partial(AdamW, betas=(0.95,0.99))\n",
    "\n",
    "%time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), loss_scale=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b224bc31df43d4b3e942dda90601ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.103511   1.215768   0.5703    \n",
      "    1      0.841045   0.88111    0.6917                      \n",
      "    2      0.69069    0.688277   0.7596                      \n",
      "    3      0.591199   0.75532    0.7447                      \n",
      "    4      0.547122   0.61999    0.7897                      \n",
      "    5      0.505276   0.722243   0.7693                      \n",
      "    6      0.45382    0.522073   0.8277                      \n",
      "    7      0.423857   0.437215   0.8522                      \n",
      "    8      0.409121   0.456454   0.8457                      \n",
      "    9      0.366972   0.407509   0.8625                      \n",
      "    10     0.357987   0.487602   0.8424                      \n",
      "    11     0.348313   0.423933   0.8587                      \n",
      "    12     0.326056   0.445858   0.8522                      \n",
      "    13     0.310947   0.3891     0.8673                      \n",
      "    14     0.277337   0.359866   0.8826                      \n",
      "    15     0.254975   0.330647   0.8935                      \n",
      "    16     0.216959   0.341561   0.8907                      \n",
      "    17     0.192235   0.327837   0.8976                      \n",
      "    18     0.162041   0.30102    0.9039                      \n",
      "    19     0.154625   0.275582   0.9115                      \n",
      "    20     0.121448   0.252004   0.9225                      \n",
      "    21     0.114184   0.280496   0.9168                       \n",
      "    22     0.091018   0.255418   0.9242                       \n",
      "    23     0.077176   0.281949   0.9194                       \n",
      "    24     0.064609   0.246405   0.9276                       \n",
      "    25     0.044858   0.262288   0.9293                       \n",
      "    26     0.035365   0.25399    0.9343                       \n",
      "    27     0.023124   0.239256   0.9358                       \n",
      "    28     0.015694   0.236261   0.9412                       \n",
      "    29     0.01214    0.224248   0.9407                       \n",
      "\n",
      "CPU times: user 7min 3s, sys: 2min 5s, total: 9min 9s\n",
      "Wall time: 9min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.224248046875, 0.9407000004768371]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=128\n",
    "sz=32\n",
    "data = torch_loader(PATH, sz, bs, 512)\n",
    "\n",
    "# m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "# m = ResNet18()\n",
    "m = wrn_22()\n",
    "\n",
    "# m = FP16(m.cuda())\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = F.cross_entropy\n",
    "# learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-4\n",
    "lr=3e-3\n",
    "# learn.clip = 1e-2\n",
    "\n",
    "learn.opt_fn = partial(AdamW, betas=(0.95,0.99))\n",
    "\n",
    "%time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), loss_scale=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to fastai AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c1e2ef39914a4f8aed771b29aa1e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.115529   1.183455   0.5851    \n",
      "    1      0.84254    0.809567   0.7215                      \n",
      "    2      0.674543   0.728618   0.7516                      \n",
      "    3      0.610069   0.789974   0.7401                      \n",
      "    4      0.552389   0.611062   0.7936                      \n",
      "    5      0.489703   0.522124   0.8199                      \n",
      "    6      0.450236   0.579267   0.8062                      \n",
      "    7      0.411513   0.623684   0.8015                      \n",
      "    8      0.393244   0.482875   0.8366                      \n",
      "    9      0.354747   0.407091   0.8629                      \n",
      "    10     0.32673    0.414187   0.8607                      \n",
      "    11     0.315894   0.433505   0.861                       \n",
      "    12     0.294576   0.479015   0.8528                      \n",
      "    13     0.28874    0.357954   0.8825                      \n",
      "    14     0.25295    0.321704   0.8945                      \n",
      "    15     0.205413   0.319233   0.8997                      \n",
      "    16     0.196892   0.29181    0.9078                      \n",
      "    17     0.170574   0.293126   0.9066                      \n",
      "    18     0.13631    0.304435   0.9072                      \n",
      "    19     0.119878   0.292626   0.9142                      \n",
      "    20     0.100636   0.28748    0.9192                       \n",
      "    21     0.076979   0.273266   0.9235                       \n",
      "    22     0.063389   0.262364   0.9293                       \n",
      "    23     0.051691   0.287785   0.9268                       \n",
      "    24     0.042461   0.281033   0.9321                       \n",
      "    25     0.032314   0.28866    0.933                        \n",
      "    26     0.020161   0.288926   0.9344                       \n",
      "    27     0.017329   0.282703   0.9384                       \n",
      "    28     0.011061   0.282318   0.9395                       \n",
      "    29     0.007818   0.276807   0.9402                        \n",
      "\n",
      "CPU times: user 7min 4s, sys: 2min 4s, total: 9min 9s\n",
      "Wall time: 9min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27680693359375, 0.9401999993324279]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=128\n",
    "sz=32\n",
    "data = torch_loader(PATH, sz, bs, 512)\n",
    "\n",
    "# m = PreActResNet(PreActBlock, [2,2,2,2], concatpool=True)\n",
    "# m = ResNet18()\n",
    "m = wrn_22()\n",
    "\n",
    "# m = FP16(m.cuda())\n",
    "learn = Learner.from_model_data(m, data)\n",
    "learn.half()\n",
    "learn.crit = F.cross_entropy\n",
    "# learn.opt_fn = optim.Adam\n",
    "learn.metrics = [accuracy]\n",
    "wd=1e-4\n",
    "lr=3e-3\n",
    "# learn.clip = 1e-2\n",
    "\n",
    "learn.opt_fn = partial(AdamWLR, betas=(0.95,0.99))\n",
    "\n",
    "%time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), loss_scale=512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "266px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
